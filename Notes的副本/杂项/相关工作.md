# 主动学习，小样本命名实体识别任务



1. 迁移学习
2. 元学习
3. 半监督学习
4. 主动学习



命名实体识别（Named Entity Recognition，简称NER）是自然语言处理领域中的一个任务，它的目标是从文本中识别出具有特定意义的实体，例如人名、地名、组织机构名、日期、时间、货币金额等。命名实体识别作为自然语言处理的一项关键任务，在许多应用中具有重要价值。经典的模型有LSTM-CRF（1 huang，2015）然而，面对有限的标注数据，传统的实体识别方法往往难以取得良好的性能。针对少样本 NER 的方法可分为四类：迁移学习、元学习、半监督学习和主动学习。

迁移学习是一种利用提高下游任务在有限标注数据下性能的流行方法。大规模预训练语言模型，如 BERT（2 Devlin 等，2018）、GPT（3 Radford 等，2018）和 RoBERTa（4 Liu 等，2019），在 NER 任务中取得了显著的改进。随后有一些少样本学习技术，如（5 Hou 等，2021）提出了 prototype merging方法，（6 Cui 2021）使用的 cloze prompt 方法和在此基础上（7 Ma，2021）提出的 template-less 范式对这些模型进行微调可进一步提高性能。

元学习，也称为学习学习，是一种学习模型初始参数的方法，然后可以使用少量标注示例对其进行微调，以快速适应新任务。原型网络（ 8 Snell 等，2017）和模型无关元学习（MAML）（9 Finn 等，2017）是应用于少样本 NER 任务的著名元学习方法。在此基础之上，（10 Ma et al，2022）提出将这两者结合的模型获得了更好的初始化参数，

半监督学习是一种同时利用有标签和无标签数据进行模型训练的方法。它在  NER 领域中的应用已经取得了一定的成功。半监督学习方法主要包括自监督学习、生成对抗网络（GANs）和协同训练等。自监督学习通过训练模型生成或预测未标注数据的某些部分来学习数据的内部结构。生成对抗网络（GANs）（11 Goodfellow et al., 2014）它们通过在生成器和判别器之间进行对抗训练来利用未标注数据以解决小样本的问题，也取得了一定的成果。

主动学习是一种迭代选择来自大量未标注数据的最具信息量实例进行注释的方法，以提高模型性能并最小化注释工作。主动学习已经有效地应用于 NER 任务，（12 Liu，2020）提出了基于传统 CRF 方法的 LTP 模型将其与主动学习结合，（13 Radmard，2021）提出的基于子序列的方法，深挖语言的序列关系和每个样本间的同构关系并传播到其他句子中，都取得了不错的成果。



Named Entity Recognition (NER) constitutes a critical task within the domain of natural language processing, which seeks to pinpoint entities of particular significance embedded in text, such as personal names, locations, organizational designations, dates, times, and monetary quantities, among others. As a pivotal component of natural language processing, NER is of immense value across a wide array of applications. Classic models, such as LSTM-CRF (1 Huang, 2015), have been developed in this area; however, conventional entity recognition methods often encounter difficulties in achieving satisfactory performance when faced with limited annotated data. Strategies for addressing few-shot NER can be classified into four primary categories: transfer learning, meta-learning, semi-supervised learning, and active learning.

Transfer learning represents a popular approach for enhancing the performance of downstream tasks under the constraint of limited annotated data. Large-scale pre-trained language models, including BERT (2 Devlin et al., 2018), GPT (3 Radford et al., 2018), and RoBERTa (4 Liu et al., 2019), have yielded considerable advancements within the realm of NER tasks. In the wake of these developments, several few-shot learning techniques have emerged, such as the prototype merging method put forth by (5 Hou et al., 2021), the cloze prompt method employed by (6 Cui, 2021), and the template-less paradigm introduced by (7 Ma, 2021). These techniques can further augment the performance of the aforementioned models through fine-tuning.

Meta-learning, alternatively referred to as learning to learn, is a technique that involves learning initial model parameters, which can subsequently be fine-tuned with a minimal number of annotated examples to rapidly adapt to new tasks. Prototype networks (8 Snell et al., 2017) and model-agnostic meta-learning (MAML) (9 Finn et al., 2017) stand as prominent meta-learning methods applied to few-shot NER tasks. Building upon these foundations, (10 Ma et al., 2022) proposed a model that integrates both approaches to attain superior initial parameters.

Semi-supervised learning is a methodology that leverages both labeled and unlabeled data during model training. This approach has already achieved a certain degree of success within the field of NER. The principal methods encompassed by semi-supervised learning include self-supervised learning, Generative Adversarial Networks (GANs), and co-training, among others. Self-supervised learning is predicated on training models to generate or predict certain portions of unlabeled data to discern the inherent structure of said data. Generative Adversarial Networks (GANs) (11 Goodfellow et al., 2014) have also yielded promising results by employing adversarial training between generators and discriminators to exploit unlabeled data in addressing the challenges posed by small sample sizes.

Active learning is an iterative technique that selects the most informative instances from a vast pool of unlabeled data for annotation in order to bolster model performance and minimize annotation efforts. Active learning has been effectively applied to NER tasks, as evidenced by (12 Liu, 2020), who proposed the LTP model based on traditional CRF methods in conjunction with active learning. Additionally, the subsequence-based approach suggested by (13 Radmard, 2021) delves deeply into the sequential relationships of language and the isomorphic relationships between individual samples, subsequently propagating these relationships to other sentences, ultimately yielding favorable outcomes.



1 Huang, Zhiheng, Wei Xu, and Kai Yu. "Bidirectional LSTM-CRF models for sequence tagging." *arXiv preprint arXiv:1508.01991* (2015).

2 Devlin, Jacob, et al. "Bert: Pre-training of deep bidirectional transformers for language understanding." *arXiv preprint arXiv:1810.04805* (2018).

3 Radford, Alec, et al. "Improving language understanding by generative pre-training." (2018).

4 Liu, Yinhan, et al. "Roberta: A robustly optimized bert pretraining approach." *arXiv preprint arXiv:1907.11692* (2019).

5 Hou, Yutai, et al. "Learning to bridge metric spaces: Few-shot joint learning of intent detection and slot filling." *arXiv preprint arXiv:2106.07343* (2021).

6 Leyang Cui, Yu Wu, Jian Liu, Sen Yang, and Yue Zhang. 2021. [Template-Based Named Entity Recognition Using BART](https://aclanthology.org/2021.findings-acl.161). In *Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021*, pages 1835–1845, Online. Association for Computational Linguistics.

7 Ma, Ruotian, et al. "Template-free prompt tuning for few-shot NER." *arXiv preprint arXiv:2109.13532* (2021).

8 Snell, Jake, Kevin Swersky, and Richard Zemel. "Prototypical networks for few-shot learning." *Advances in neural information processing systems* 30 (2017).

9 Finn, Chelsea, Pieter Abbeel, and Sergey Levine. "Model-agnostic meta-learning for fast adaptation of deep networks." *International conference on machine learning*. PMLR, 2017.

10 Ma, Tingting, et al. "Decomposed Meta-Learning for Few-Shot Named Entity Recognition." *arXiv preprint arXiv:2204.05751* (2022).

11 [1] Goodfellow, I. J. , et al. "Generative Adversarial Networks." (2014).

12 Liu, Mingyi, et al. "LTP: a new active learning strategy for BERT-CRF based named entity recognition." *arXiv preprint arXiv:2001.02524* (2020).

13 Radmard, Puria, Yassir Fathullah, and Aldo Lipani. "Subsequence based deep active learning for named entity recognition." *Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)*. 2021.