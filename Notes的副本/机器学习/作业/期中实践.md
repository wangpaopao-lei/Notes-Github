# 第一问

机器学习的任务大致包含如下阶段

## 1 数据预处理

---

1. 统一量纲（本次任务没有体现）
2. 类别型特征转换
   - 采用编码方式，如 One-Hot 编码
3. 缺失值处理
   - 对缺失率超过一定阈值的特征进行删除
   - 对缺失数量少的特征用前一个非缺失值填充
   - 对其他类别的缺失数据用 None 填充
   - 对数值型缺失值补 0
   - ==此外，还有其他方法可以用于缺失值处理，如忽略缺失元组、填充中心度量（中位数、众数）、填充可能的值 等==
4. 光滑噪声
   - 本次任务采用了离群点分析方法
   - ==此外，还有其他方法，如分箱法、回归法==
5. 异常值处理
   - 包含检测和处理两个部分
   - 常用的检测方法有箱型图、基于模型检测、基于距离、基于密度、基于聚类
   - 常用的处理方法有：
     - 直接删除
     - 视为缺失值，使用缺失值处理方法
     - 采用平均值来修正
6. 数据标准化
   - 本次任务采用了 RobustScaler 方法
   - ==此外还有 min-max、z-score 等标准化方法==

## 2 数据探索

---

1. 分布分析，
   - 如直方图、偏度峰度分析
2. 统计指标分析
   - 如最大 最小值、平均值、25% 50% 75%值分析
3. 周期性分析（本次任务没有体现）
4. 关联分析
   - 如相关性矩阵、画图分析

## 3 特征工程

---

1. 特征选择
   - 删除高度相关或重复率超过 85%的特征
   - ==除此之外还可以使用一些数据降维方法，如 PCA 主成分分析==
   - 以上讨论的是==维归约==方式，除此之外还有数量归约、数据压缩等方式
2. 特征表示
   - 数值型转类别型
   - 类别型 one-hot 编码
   - 对数变换，正态分布调整

## 4 数据建模



# 第二问

如果不进行特征选择会怎样？

---

#### 1 猜测：

1. 分析或挖掘复杂数据需要花费非常长的时间
2. 可能导致过拟合

#### 2 实践：

对程序做如下修改：

![image-20221202221549857](https://wangleidetuchuang.oss-cn-beijing.aliyuncs.com/img/image-20221202221549857.png)

***注释掉特征选择部分代码，运行，以下展示部分结果***



**在 XGBRegressor 上**的表现==差别不大==

<img src="https://wangleidetuchuang.oss-cn-beijing.aliyuncs.com/img/image-20221202221708671.png" alt="image-20221202221708671" style="zoom:25%;" />

**而在层叠回归模型上**发生了明显的==过拟合==

<img src="https://wangleidetuchuang.oss-cn-beijing.aliyuncs.com/img/image-20221202222027282.png" alt="image-20221202222027282" style="zoom:25%;" />



# 第三问

多层神经网络的过拟合实验

---

#### 1 原始模型

```python
clf = MLPRegressor(activation="tanh",
                   solver='lbfgs',alpha=1e-5, 
                   hidden_layer_sizes=(50,20), 
                   max_iter=20000,
                   batch_size=20,
                   random_state=1)
```

由上至下参数介绍：

1. 激活函数：tanh
2. 参数优化算法：lbfgs（拟牛顿法）
3. L2 正则化参数：0.00001
4. 隐藏层层数、节点数：50 层，每层 20 个单元
5. 最大迭代轮数：20000
6. 批大小：20

***查阅文档可知，当不指定损失值阈值的时候，会一直迭代到最大轮数再停止，==因此我们可以通过仅更改迭代次数来控制模型是否过拟合==***，分别取不同的迭代次数，运行结果如下

20000

<img src="https://wangleidetuchuang.oss-cn-beijing.aliyuncs.com/img/image-20221202231630805.png" alt="image-20221202231630805" style="zoom: 25%;" />

2000

<img src="https://wangleidetuchuang.oss-cn-beijing.aliyuncs.com/img/image-20221203011331889.png" alt="image-20221203011331889" style="zoom:25%;" />

1000

<img src="https://wangleidetuchuang.oss-cn-beijing.aliyuncs.com/img/image-20221203011817864.png" alt="image-20221203011817864" style="zoom:25%;" />

500

<img src="https://wangleidetuchuang.oss-cn-beijing.aliyuncs.com/img/image-20221203011718437.png" alt="image-20221203011718437" style="zoom:25%;" />

200

<img src="https://wangleidetuchuang.oss-cn-beijing.aliyuncs.com/img/image-20221203011408562.png" alt="image-20221203011408562" style="zoom:25%;" />

150

<img src="https://wangleidetuchuang.oss-cn-beijing.aliyuncs.com/img/image-20221203011557175.png" alt="image-20221203011557175" style="zoom:25%;" />

100

<img src="https://wangleidetuchuang.oss-cn-beijing.aliyuncs.com/img/image-20221203011510492.png" alt="image-20221203011510492" style="zoom:25%;" />

#### 实验结果分析

可以看到，迭代次数从 100 增加到 500 的过程中，随着迭代次数的增加，训练集误差逐渐变小，同时验证集的误差也在变小；而在迭代次数从 500 增加到 20000 的过程中，随着训练集误差的减小，验证集误差反而不断变大。==因此可以初步判断，在 200——1000 这个 `max_iter ` 区间内，模型已经发生了过拟合==。

<img src="https://wangleidetuchuang.oss-cn-beijing.aliyuncs.com/img/image-20221203012938883.png" alt="image-20221203012938883" style="zoom:25%;" />

作为横向对比，在设定迭代次数为 1000 的基础上，将隐层的层数由 50 增加到 100，隐层节点数不变（增加模型复杂度），来通过另一种方式让模型达到过拟合。==结果显示，相较于原始的 1000 次迭代模型，更复杂的模型也发生了过拟合==
