![image-20220628125525737](https://wangleidetuchuang.oss-cn-beijing.aliyuncs.com/img/image-20220628125525737.png)

# 数据预处理

---

## `int`特征：

1. `income` 收入
   - 有一万多个缺失值
     - 均值填充
2. `age` 年龄
   - 有两万多个年龄为 0
     - 视作缺失值
     - 均值填充
3. `experience`职业经历
4. `currentjobyears`最近的工作的工作年份
5. `currenthouseyears`最近的房子的居住年份
6. `ID`——无关特征，删除

## `object`特征

1. `married`已婚
   - 已婚 1，未婚 0
2. `house_ownership`房子拥有权
   - onehot 编码
3.  `car_ownership`车拥有权
   - 无车 0，有车 1
4. `city`城市、`state`州、`profession`职业
   - 分类特征编码：`count_encoder`

# 建模

---

## 实例化





## 通过模型接口训练模型





## 通过模型接口提取需要的信息

predict_proba接口



## 调参

```python
start=time.time()
scorel = []
for i in range(0,200,10): # 迭代建立包含0-200棵决策树的RF模型进行对比
    rfc = RandomForestClassifier(n_estimators=i+1,n_jobs=-1,random_state=90)
    score = cross_val_score(rfc,X,Y,cv=10).mean()
    scorel.append(score)
print(max(scorel),(scorel.index(max(scorel))*10)+1)
end=time.time()
print('Running time: %s Seconds'%(end-start))
plt.figure(figsize=[20,5])
plt.plot(range(1,201,10),scorel)
plt.show()
```

计算 AUC 值

```python
from sklearn.metrics import roc_auc_score
y_score = clf.predict_proba(X_test)
print(f"AUC={roc_auc_score(y_test, y_score, average='micro')}")
```

![image-20220628173132197](https://wangleidetuchuang.oss-cn-beijing.aliyuncs.com/img/image-20220628173132197.png)

由默认的 0.89694 提升到了 0.89878

![image-20220628175911998](https://wangleidetuchuang.oss-cn-beijing.aliyuncs.com/img/image-20220628175911998.png)

提交答案，结果成绩只有 0.52



发现问题：

分割训练集进行测试时结果有很多 0，也有很多 0.4 以上的数据

使用测试集时几乎全部都是 0.1——0.4 之间的数据

![image-20220628231235333](https://wangleidetuchuang.oss-cn-beijing.aliyuncs.com/img/image-20220628231235333.png)

![image-20220628231243653](https://wangleidetuchuang.oss-cn-beijing.aliyuncs.com/img/image-20220628231243653.png)

我们分析训练集和测试集的数据分布情况，排除了数据分布不均的情况

![image-20220629083541667](https://wangleidetuchuang.oss-cn-beijing.aliyuncs.com/img/image-20220629083541667.png)

将 n_estimators 设置为 5，发现数据分布与分割训练集的结果比较像，推测可能出现过拟合情况



重新调参，再次提交分数更低……放弃



# LightGBM

使用 sklearn 的接口

## 调参

---

### boosting_type 基学习器模型

给定第一组参数试运行，发现速度非常快，得分也比较高

基学习器设为 `gdbt`：梯度提升决策树

![image-20220630005753785](https://wangleidetuchuang.oss-cn-beijing.aliyuncs.com/img/image-20220630005753785.png)



将基学习器换为 rf 随机森林

![image-20220630010013329](https://wangleidetuchuang.oss-cn-beijing.aliyuncs.com/img/image-20220630010013329.png)

将基学习器设为 `dart`：带 dropout 的梯度提升决策树

![image-20220630010244268](https://wangleidetuchuang.oss-cn-beijing.aliyuncs.com/img/image-20220630010244268.png)

将基学习器设为 `goss`：表示`Gradient-based One-Side Sampling` 的`gbdt`

同时删除 `bagging_fraction`参数

![image-20220630010540587](https://wangleidetuchuang.oss-cn-beijing.aliyuncs.com/img/image-20220630010540587.png)

相较于传统 gdbt，结果略有提升

### max_depth, num_leaves

确定树的大小和复杂度，采用网格搜索

第一次参数

![image-20220630013530978](https://wangleidetuchuang.oss-cn-beijing.aliyuncs.com/img/image-20220630013530978.png)

结果![image-20220630013555434](https://wangleidetuchuang.oss-cn-beijing.aliyuncs.com/img/image-20220630013555434.png)

结果显示仍有上升空间



第二次参数

![image-20220630014018154](https://wangleidetuchuang.oss-cn-beijing.aliyuncs.com/img/image-20220630014018154.png)

仍是最大值时最优,还有提升空间

![image-20220630013949213](https://wangleidetuchuang.oss-cn-beijing.aliyuncs.com/img/image-20220630013949213.png)



第三次参数

![image-20220630014209046](https://wangleidetuchuang.oss-cn-beijing.aliyuncs.com/img/image-20220630014209046.png)

结果还在上升

![image-20220630014632431](https://wangleidetuchuang.oss-cn-beijing.aliyuncs.com/img/image-20220630014632431.png)

继续放大参数

![image-20220630020005933](https://wangleidetuchuang.oss-cn-beijing.aliyuncs.com/img/image-20220630020005933.png)

![image-20220630021011974](https://wangleidetuchuang.oss-cn-beijing.aliyuncs.com/img/image-20220630021011974.png)

此时可以将 `max_depth`定为 17，继续调整 `num_levels`

![image-20220630021836477](https://wangleidetuchuang.oss-cn-beijing.aliyuncs.com/img/image-20220630021836477.png)

![image-20220630022858011](https://wangleidetuchuang.oss-cn-beijing.aliyuncs.com/img/image-20220630022858011.png)

![image-20220630023351199](https://wangleidetuchuang.oss-cn-beijing.aliyuncs.com/img/image-20220630023351199.png)

最终确定为 17,260

==按照上述方法依次对以下参数进行调试==

### bagging_fraction, bagging_freq

前者相当于采样比例，后者表示每 k 轮迭代进行一次 bagging

![image-20220630025057119](https://wangleidetuchuang.oss-cn-beijing.aliyuncs.com/img/image-20220630025057119.png)

### cat_smooth

每个类别拥有的最小个数，主要用于去噪

![image-20220630025520991](https://wangleidetuchuang.oss-cn-beijing.aliyuncs.com/img/image-20220630025520991.png)





最终参数确定：

```python
gbm = lgb.LGBMClassifier(objective = 'binary',
                         is_unbalance = True,
                         metric = 'binary_logloss,auc',
                         max_depth = 17,
                         num_leaves = 260,
                         learning_rate = 0.1,
                         feature_fraction = 0.7,
                         min_child_samples=21,
                         min_child_weight=0.001,
                         bagging_fraction = 0.9,
                         bagging_freq = 2,
                         reg_alpha = 0.001,
                         reg_lambda = 8,
                         cat_smooth = 0,
                         num_iterations = 200,
                         )
```



## 问题及改进思路

---

1. 参数的局部最优不一定是全局最优
   - 使用维度更高的网格进行搜索，相应的，调试时间会大大增加
2. 
