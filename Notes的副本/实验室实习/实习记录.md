# 4.28

### 李沐

1. 图能表示什么样的数据
2. 图和其他数据类型的区别
3. 构造一个现代 GNN，跑完模型每个层
4. playground

---

Relations(edges) between entities(nodes)

三个问题

1. 图层面
2. 顶点层面
3. 边层面

---

机器学习用在图上的挑战

==如何存储？==

图包含四种信息

1. 顶点属性
2. 边属性
3. 全局属性
4. 连接性

要求

1. 存储高效
2. 排序不影响信息

方法

1. 每个点用标量表示
2. 边用标量表示
3. 全局信息用标量表示
4. 维护邻接列表
   * 第 i 个数据对应第 i 条边连接的顶点

---

GNN

对图上所有属性进行可以优化的变换（能保持图的对称信息）

使用 message-passing 框架

输入是一个图输出也是一个图，对属性进行变换，但是不会改变图的connectivity

三层多层感知器 MLP，分别对应点、边、全局的向量的映射作为更新

 汇聚：例如缺失某个顶点数据时，将其边数据拿过来做预测

池化

---

# 4.30

**《异质图神经网络开原算法库》**

![image-20220510110505748](https://wangleidetuchuang.oss-cn-beijing.aliyuncs.com/img/image-20220510110505748.png)

稀疏矩阵计算

数据依赖：需要 n 层以外的信息，需要进行采样

OpenHGNN：![image-20220510111220188](https://wangleidetuchuang.oss-cn-beijing.aliyuncs.com/img/image-20220510111220188.png)

一键跑完

能定制数据集

高效

![image-20220510111754235](https://wangleidetuchuang.oss-cn-beijing.aliyuncs.com/img/image-20220510111754235.png)

![image-20220510112208223](https://wangleidetuchuang.oss-cn-beijing.aliyuncs.com/img/image-20220510112208223.png)

# 5.5

生成器和判别器的对抗

![image-20220505150651856](https://wangleidetuchuang.oss-cn-beijing.aliyuncs.com/img/image-20220505150651856.png)

听完论文分享，对 GAN 有点兴趣，遂找来李宏毅的课程

共分三个课时，只看了一个半

---

### Generator 的表示

$$
G^{*}=\arg \min _{G} \underline{\operatorname{Div}}\left(P_{G}, P_{\text {data }}\right)
$$

divergence可以用来衡量生成与真实的相似度

![image-20220510114953229](https://wangleidetuchuang.oss-cn-beijing.aliyuncs.com/img/image-20220510114953229.png)

divergence比较小时，真实图像与生成图像的差距小，discriminator就比较难鉴别，就会拥有比较小的红框框数值。divergence比较大时，discriminator就比较容易鉴别，就会拥有比较大的红框框数值。**因为maxV（D,G）与divergence有关，所以可以用maxV（D,G）代替Div（PG，Pdata），这样就实现了不用了解PG和Pdata具体样貌即可计算divergence。**改变objective function即V（D,G）就可以计算其他类型的divergence了

---

# 5.7-5.8

##  Python 实现回归算法

### 一元线性回归
1. 首先假设 $f(x_i)=z_i=wx_i+b$

2. 定义损失函数为均方误差$$
  $$
  M S E=\frac{\sum_{i=1}^{n}\left(y_{i}-\widehat{y_{i}}\right)^{2}}{n}, \quad \widehat{y_{i}}=f\left(x_{i}\right) \tag{1.1}
  $$
  
$$
3. 求损失函数分别对 w 和 b 的偏导数$$\frac{\partial \mathrm{E}_{(w, b)}}{\partial w}=2\left(\mathrm{w} \sum_{i=1}^{n} x_{i}^{2}-\sum_{i=1}^{n}\left(y_{i}-b\right) x_{i}\right) \tag{1.2}
$$
$$
\frac{\boldsymbol{\partial E}_{(w, b)}}{\partial b}=2\left(n b-\sum_{i=1}^{n}\left(y_{i}-w x_{i}\right)\right) \tag{1.3}
$$
4. 令偏导数为 0，求得最优解$$
  $$
  w=\frac{\sum_{i=1}^{n}\left(x_{i}-\bar{x}\right) y_{i}}{\sum_{i=1}^{n} x_{i}^{2}-\frac{1}{n}\left(\sum_{i=1}^{n} x_{i}\right)^{2}} \tag{1.4}
  $$
  

$$
$$b=\frac{1}{n} \sum_{i=1}^{n}\left(y_{i}-w x_{i}\right)\tag{1.5}
$$

---

### 多元线性回归 

根据先前所计算的相关性，选择相关性较高的三个特征量： LSTAT（低社会地位人口占比）、INDUS（非零售业占比）、PTRATIO（学生-老师数量比）作为回归数据

### 2. 使用标准方程法进行多元线性回归
1. 首先假设：$\mathrm{f}\left(x_{i}\right)=\mathrm{w}^{T} x_{i}+b$

2. 改写为：
  $$\mathrm{f}\left(x_{i}\right)=\mathrm{w}_{1} x_{i 1}+w_{2} x_{i 2}+\cdots+w_{d} x_{i d}+\mathrm{w}_{0} x_{0}\tag{2.1}$$

3. 其中 x0 恒为 1，因此，原本$n\times d$的数据集要改写为$n\times (d+1)$

4. 设损失函数为 
   $$
   E(w)=(y-X \widehat{w})^{T}(y-X \widehat{w})\tag{2.2}
   $$
   

5. 求导得
   $$
   \frac{\partial E_{(\widehat{w})}}{\partial \widehat{w}}=2 X^{T}(X \widehat{W}-y)\tag{2.3}
   $$

6. 求得 w 最优解
   $$
   \widehat{\mathrm{w}}=\left(X^{T} X\right)^{-1} X^{T} y\tag{2.4}
   $$
   

---

### 逻辑回归

把线性回归的假设函数$z=X\theta$作为 x 传入其中

得到逻辑回归的假设函数
$$
h_{\theta}(x)=g\left(\theta^{T} x\right)$$
$$


$$
h_{\theta}(x)=\frac{1}{1+\mathrm{e}^{-\theta^{T} X}}\tag{3.1}$$
$$


h(x)的值，是样本属于 1 类别的概率

---

### 3. 定义损失函数
如果用最小二乘法，目标函数是
$$
E_{w, b}=\sum_{i=1}^{m}\left(y_{i}-\frac{1}{1+e^{-\left(w^{T} x_{i}+b\right)}}\right)^{2}
$$
显然该函数是非凸的，不利于求解
因此使用对数损失函数
$$
cost\_function\left(h_{\theta}(x), y\right)= \begin{cases}-\log \left(h_{\theta}(x)\right. & \text { if } y=1 \\ -\log \left(1-h_{\theta}(x)\right) & \text { if } y=0\end{cases}$$
$$

组合起来写就是
$$
J(\theta)=-\frac{1}{m} \sum_{i=1}^{m} y i \cdot \log \left(h_{\theta}\left(x^{i}\right)\right)+(1-y i) \log \left(1-h_{\theta}\left(x^i\right)\right)$$
$$


### 4. 最大似然估计求解参数
求得
$$
\max \sum_{i=1}^{M}\left(y_{i} \log \cdot P_{1}+(1-y) \log \cdot P_{0}\right)
\\=\min -\frac{1}{m} \sum_{i=1}^{m}\left(y_{i} \cdot \log \left(h_{\theta}\left(x_{i}\right)\right)+\left(1-y_{i}\right) \cdot \log \left(1-h_{\theta}\left(x^{i}\right)\right)\right)
$$

### 5. 对损失函数推导梯度
$$
\frac{\partial J(\theta)}{\partial \theta}=\boldsymbol{X}^{\boldsymbol{T}}\left(p_{\theta}(X)-Y\right)\tag{3.2}
$$



### 6. 得到梯度下降的迭代公式
$$
\theta=\theta-\alpha X^{\boldsymbol{T}}\left(p_{\theta}(X)-Y\right)\tag{3.3}$$
$$


$\alpha$为学习率（步长）
