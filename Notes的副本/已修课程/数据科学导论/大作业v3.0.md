

![image-20220701092924331](https://wangleidetuchuang.oss-cn-beijing.aliyuncs.com/img/image-20220701092924331.png)

<div STYLE="page-break-after: always;">

![image-20220701093017538](https://wangleidetuchuang.oss-cn-beijing.aliyuncs.com/img/image-20220701093017538.png)


# 目录

[toc]

<div STYLE="page-break-after: always;">



## 任务描述

当借款人从贷款机构借钱而不能如期还贷款时，就可能会发生贷款违约。拖欠贷款不仅会上报征信，还可能有被起诉的风险。

为更好的管控风险，贷款机构通常会基于用户信息来预测用户贷款是否违约，本作业便是使用示例数据集来尝试解释预测贷款违约的工作原理。

训练数据集共 201600 条数据，包含 12 个特征，1 个标签。

测试数据集共 50400 条数据，包含 12 个特征，不含标签。	




## 数据探索



### 概览

<img src="https://wangleidetuchuang.oss-cn-beijing.aliyuncs.com/img/image-20220701093838655.png" alt="image-20220701093838655" style="zoom: 33%;" />



结合文档中所述的各字段含义，该数据集有 7 个数值特征，6 个类别特征。首先我们注意到 ID 字段，我们猜测在数据集中属于各不相同的特征，在后面的处理中应该删除；然后 Income 字段出现了缺失值，后续要进行缺失值处理

### 数值特征可视化分析

<img src="https://wangleidetuchuang.oss-cn-beijing.aliyuncs.com/img/image-20220701094250688.png" alt="image-20220701094250688" style="zoom:33%;" />



我们注意到标签中，有逾期风险的样本只占很小一部分；我们注意到 Id 的分布，进一步验证了前文中所述的与本次任务无关的猜测；同时也注意到，Age 特征出现了大量 0 附近的异常值。

<div STYLE="page-break-after: always;">

## 两次尝试

本次实验我们共用了两种模型进行学习，分别是随机森林分类器和 LightGBM 算法，在前一次的尝试中，我们的随机森林模型出现了分割训练集的预测结果与测试集的预测结果相差巨大的情况，挖掘原因后，我们选择使用效率与准确性更高的 LightGBM 模型，并完成了实验。

### 随机森林

随机森林的基学习器选用决策树，对训练样本和特征进行抽样，抽样方法使用 Boosttrap，预测结果为子树结果的平均值；相比单棵的决策树，RF 模型的边界更加平滑，置信区间也更大。

#### 数据预处理

随机森林不需要数值数据归一化，因此数据预处理的任务集中在缺失值处理部分。

<img src="https://wangleidetuchuang.oss-cn-beijing.aliyuncs.com/img/8D38DC1A-3825-4BB5-AAA6-3F0602DA4146.png" alt="8D38DC1A-3825-4BB5-AAA6-3F0602DA4146" style="zoom:33%;" />

包含缺失值的特征不仅有直观上的 Income 字段，还包含 Age 里数量众多的 ‘0’ ，因此首先将 Age 里的 0 替换为空值 ‘NaN’ ，再进行缺失值填充，我们选用平均值填充的方法。

#### 特征工程

数值特征已处理完毕，还需对类别特征进行处理。对婚姻和车辆拥有情况这类二值数据进行 01 编码，房屋拥有权这项特征有 3 种取值，进行 one-hot 编码，其余数据使用分类特征编码。

#### 建模

使用 sklearn 中的 `RandomForestClassifier` 模型。

主要参数确定为 `n_estimators` 字段，该字段指明随机森林中决策树的数量，从 1 到 200 建立模型进行对比

```python
start=time.time()
scorel = []
for i in range(0,200,10): # 迭代建立包含0-200棵决策树的RF模型进行对比
    rfc = RandomForestClassifier(n_estimators=i+1,n_jobs=-1,random_state=90)
    score = cross_val_score(rfc,X,Y,cv=10).mean()
    scorel.append(score)
print(max(scorel),(scorel.index(max(scorel))*10)+1)
end=time.time()
print('Running time: %s Seconds'%(end-start))
plt.figure(figsize=[20,5])
plt.plot(range(1,201,10),scorel)
plt.show()
```

<img src="https://wangleidetuchuang.oss-cn-beijing.aliyuncs.com/img/image-20220701111146553.png" alt="image-20220701111146553" style="zoom:50%;" />

最终确定参数为

````python
n_estimators=151,
criterion='gini',
random_state=90
````

#### 预测

通过 sklearn 的 predict_proba 接口提取分类概率信息，提交结果。

#### 结果分析

结果显示 AUC 值只有 0.52 左右，严重低于预期，我们分析了一下可能原因：

1. 训练集与测试集样本分布偏差严重

   - 我们注意到训练集与测试集的分类概率取值区间存在很大差别，因此估计样本的类别分布出现偏差

   - <img src="https://wangleidetuchuang.oss-cn-beijing.aliyuncs.com/img/image-20220701111431941.png" alt="image-20220701111431941" style="zoom:33%;" />

   - 结果显示，分布基本相同

2. 模型存在过拟合情况
   - 我们将最大决策树数量调整至 5 棵，概率分布出现明显的好转，因此基本判断模型出现了过拟合，但是以现有手段，我们难以解决过拟合的修正问题，不得不放弃该模型

<div STYLE="page-break-after: always;">

### LightGBM(Light Gradient Boosting Machine)

LightGBM 是微软 2017 年提出的，是对传统 GBDT（梯度提升决策树）算法的改进，改进体现在以下方面：

1. Gradient-based One-Side Sampling(GOSS)技术是去掉了很大一部分梯度很小的数据，只使用剩下的去估计信息增益，避免低梯度长尾部分的影响。由于梯度大的数据在计算信息增益的时候更重要，所以GOSS在小很多的数据上仍然可以取得相当准确的估计值。
2. Exclusive Feature Bundling(EFB)技术是指捆绑互斥的特征(i.e.， 从不同时取非零值)，以减少特征的数量。但对互斥特征寻找最佳的捆绑方式是一个NP难问题，当时贪婪算法可以取得相当好的近似率(因此可以在不显著影响分裂点选择的准确性的情况下，显著地减少特征数量)。
3. 在传统GBDT算法中，最耗时的步骤是找到最优划分点，传统方法是Pre-Sorted方式，其会在排好序的特征值上枚举所有可能的特征点，而LightGBM中会使用histogram算法替换了传统的Pre-Sorted。基本思想是先把连续的浮点特征值离散化成k个整数，同时构造出图8所示的一个宽度为k的直方图。最开始时将离散化后的值作为索引在直方图中累积统计量，当遍历完一次数据后，直方图累积了离散化需要的统计量，之后进行节点分裂时，可以根据直方图上的离散值，从这k个桶中找到最佳的划分点，从而能更快的找到最优的分割点，而且因为直方图算法无需像Pre-Sorted那样存储预排序的结果，而只是保存特征离散过得数值，所以使用直方图的方式可以减少对内存的消耗。

#### 数据预处理

LightGBM 模型不需要对缺失值进行处理，对于数值特征不需要归一化，对于分类特征不需要进行 onehot 编码，模型自带的编码方式比 onehot 更快，故数据预处理只进行以下部分：

1. 特征名称正则化
2. 删除无关特征
3. 异常值处理
4. 分类变量使用字典转换为数字

#### 建模

指明哪些特征是分类特征

```python
categorical_feature = ['MarriedSingle', 'House_Ownership',
                       'Car_Ownership', 'Profession',
                       'CITY', 'STATE']
```

使用网格搜索确定参数

##### max_depth, num_leaves

确定树的大小和复杂度

<figure class="third">     
  <img src="https://wangleidetuchuang.oss-cn-beijing.aliyuncs.com/img/image-20220630014018154.png" width="200">     
  <img src="https://wangleidetuchuang.oss-cn-beijing.aliyuncs.com/img/image-20220630013530978.png" width="200"> 
  <img src="https://wangleidetuchuang.oss-cn-beijing.aliyuncs.com/img/image-20220630014209046.png" width="200"> 
</figure>

最终确定参数分别为17,260

##### bagging_fraction, bagging_freq

前者相当于采样比例，后者表示每 k 轮迭代进行一次 bagging

最终参数确定为 0.9,2

模型最终为

```python
gbm = lgb.LGBMClassifier(objective = 'binary',
                         is_unbalance = True,
                         metric = 'binary_logloss,auc',
                         max_depth = 17,
                         num_leaves = 260,
                         learning_rate = 0.1,
                         feature_fraction = 0.7,
                         min_child_samples=21,
                         min_child_weight=0.001,
                         bagging_fraction = 0.9,
                         bagging_freq = 2,
                         reg_alpha = 0.001,
                         reg_lambda = 8,
                         cat_smooth = 0,
                         num_iterations = 200,
                         )
```



#### 结果分析及优化思路

提交后结果为 0.87，已经有了很大的进步，但仍有提升空间，且更换模型后，训练时间也大大缩短，我们给出以下优化方向及思路：

1. 参数的局部最优不一定是全局最优，因此分组调整每组参数得到的结果不一定是最优结果。
   - 使用更大维度的网格搜索，但相应的，训练时间会大大增加
   - 采用 optuna、flaml 等自动调参库
2. 特征的选取和处理，没有对可能影响更大的特征进行加权，也没有处理可能对结果无影响的特征。
   - 调用 lightGBM 的 feature_importance 工具对特征重要性进行分析
   - 进行更深层次的数据探索

<div STYLE="page-break-after: always;">

## 源代码

```python
import numpy as np
import pandas as pd
import category_encoders as ce
import sklearn.preprocessing as skp
from sklearn.model_selection import train_test_split
from sklearn.metrics import roc_auc_score
data=pd.read_csv("./train-data.csv")

# 正则化
import re
data = data.rename(columns = lambda x:re.sub('[^A-Za-z0-9_]+', '', x))

# 删除 ID
data=data.drop(['Id'],axis=1)
# 将 Age 的 0 转换为 NaN
data['Age'].replace(0,np.NaN,inplace=True)

# 文字类型转换
data.dtypes[data.dtypes=='object']

# Married 和 Car 转换为 01 编码
label_encoder = skp.LabelEncoder()
for col in ['MarriedSingle','Car_Ownership']:
data[col] = label_encoder.fit_transform( data[col] )

# 其余文字类型转换为数字类型
House = data['House_Ownership'].value_counts().index
House_dict = dict(zip(House, [i for i in range(3)]))
data['House_Ownership']  = data['House_Ownership'].map(House_dict)

Profession = data['Profession'].value_counts().index
House_dict = dict(zip(Profession, [i for i in range(51)]))
data['Profession']  = data['Profession'].map(House_dict)

CITY = data['CITY'].value_counts().index
CITY_dict = dict(zip(CITY, [i for i in range(317)]))
data['CITY']  = data['CITY'].map(House_dict)

STATE = data['STATE'].value_counts().index
STATE_dict = dict(zip(STATE, [i for i in range(29)]))
data['STATE']  = data['STATE'].map(House_dict)
# 分割训练集
Y=data['Risk_Flag']
X=data.drop(['Risk_Flag'],axis=1)
x_train, x_test, y_train, y_test = train_test_split(X,Y, test_size = 0.25)
x_train= pd.DataFrame(x_train)
y=pd.DataFrame(y_train)
y_train=pd.Series(y_train)
from sklearn.model_selection import GridSearchCV
import lightgbm as lgb
import time
# 读入数据集
lgb_train = lgb.Dataset(x_train, y_train)
lgb_eval = lgb.Dataset(x_test, y_test, reference = lgb_train)
lgb_test=lgb.Dataset(X,Y)

# params = {
#     'task':'train',
#     'boosting_type':'goss',
#     'objective':'binary',
#     'metric':{'auc'},
#     'num_leaves':40,
#     'learning_rate':0.05,
#     'feature_fraction':0.9,
#     # 'bagging_fraction':0.8,
#     'bagging_freq':5,
#     'verbose':0,
#     'is_unbalance':True,
# }
# # 训练
# start=time.time()
# lgbc = lgb.train(params,
#                  lgb_train,
#                  num_boost_round=1000,
#                  )
#
# # 预测
# y_score=lgbc.predict(x_test)
# auc_score = roc_auc_score(y_test, y_score)
# end=time.time()
# print('Running time: %s Seconds'%(end-start))
# print(auc_score)

# # 网格搜索
# parameters = {
#     'max_depth': [15, 20, 25, 30, 35],
#     'learning_rate': [0.01, 0.02, 0.05, 0.1, 0.15],
#     'feature_fraction': [0.6, 0.7, 0.8, 0.9, 0.95],
#     'bagging_fraction': [0.6, 0.7, 0.8, 0.9, 0.95],
#     'bagging_freq': [2, 4, 5, 6, 8],
#     'lambda_l1': [0, 0.1, 0.4, 0.5, 0.6],
#     'lambda_l2': [0, 10, 15, 35, 40],
#     'cat_smooth': [1, 10, 15, 20, 35]
# }
# gbm = LGBMClassifier(max_depth=3,
#                      learning_rate=0.1,
#                      n_estimators=200, # 使用多少个弱分类器
#                      objective='multiclass',
#                      num_class=3,
#                      booster='goss',
#                      min_child_weight=2,
#                      subsample=0.8,
#                      colsample_bytree=0.8,
#                      reg_alpha=0,
#                      reg_lambda=1,
#                      seed=0 # 随机数种子
#                      )
# # 有了gridsearch我们便不需要fit函数
# gsearch = GridSearchCV(gbm, param_grid=parameters, scoring='accuracy', cv=3)
# gsearch.fit(x_train, y_train)
#
# print("Best score: %0.3f" % gsearch.best_score_)
# print("Best parameters set:")
# best_parameters = gsearch.best_estimator_.get_params()
# for param_name in sorted(parameters.keys()):
#     print("\t%s: %r" % (param_name, best_parameters[param_name]))
# start=time.time()

# parameters = {
#     'cat_smooth': [0,10,20],
# }

# gbm = lgb.LGBMClassifier(objective = 'binary',
#                          is_unbalance = True,
#                          metric = 'binary_logloss,auc',
#                          max_depth = 17,
#                          num_leaves = 260,
#                          learning_rate = 0.1,
#                          feature_fraction = 0.7,
#                          min_child_samples=21,
#                          min_child_weight=0.001,
#                          bagging_fraction = 0.9,
#                          bagging_freq = 2,
#                          reg_alpha = 0.001,
#                          reg_lambda = 8,
#                          cat_smooth = 0,
#                          num_iterations = 200,
#                          )
# gsearch = GridSearchCV(gbm, param_grid=parameters, scoring='roc_auc', cv=3)
# gsearch.fit(x_train, y_train)
# print('参数的最佳取值:{0}'.format(gsearch.best_params_))
# print('最佳模型得分:{0}'.format(gsearch.best_score_))
# end=time.time()
# print('Running time: %s Seconds'%(end-start))
# print(gsearch.cv_results_['mean_test_score'])
# print(gsearch.cv_results_['params'])
# 向模型指明分类特征
categorical_feature = ['MarriedSingle', 'House_Ownership',
                       'Car_Ownership', 'Profession',
                       'CITY', 'STATE']

# 指定参数
params = {
    'task':'train',
    'boosting_type':'gbdt',
    'objective':'binary',
    'metric':{'auc','binary_logloss'},
    'max_depth' : 17,
    'num_leaves':260,
    'learning_rate':0.05,
    'feature_fraction':0.7,
    'bagging_fraction':0.9,
    'bagging_freq':2,
    'verbose':0,
    'is_unbalance':True,
}

# 训练
lgbc=lgb.train( params,lgb_train,valid_sets=lgb_eval,categorical_feature=categorical_feature)

# 预测并评估
y_score=lgbc.predict(x_test)
print(roc_auc_score(y_test, y_score))
# 生成模型并保存
lgbcs=lgb.train( params,lgb_test,categorical_feature=categorical_feature)
lgbcs.save_model('model.txt')
# 读入
test=pd.read_csv("./test-data.csv")
# 正则化
test = test.rename(columns = lambda x:re.sub('[^A-Za-z0-9_]+', '', x))
# 删除 ID
test=test.drop(['Id'],axis=1)
# 将 Age 的 0 转换为 NaN
test['Age'].replace(0,np.NaN,inplace=True)
# MarriedSingle 和 Car 转换为 01 编码
label_encoder = skp.LabelEncoder()
for col in ['MarriedSingle','Car_Ownership']:
    test[col] = label_encoder.fit_transform( test[col] )
# 其余文字类型转换为数字类型
House = test['House_Ownership'].value_counts().index
House_dict = dict(zip(House, [i for i in range(3)]))
test['House_Ownership']  = test['House_Ownership'].map(House_dict)

Profession = test['Profession'].value_counts().index
House_dict = dict(zip(Profession, [i for i in range(51)]))
test['Profession']  = test['Profession'].map(House_dict)

CITY = test['CITY'].value_counts().index
CITY_dict = dict(zip(CITY, [i for i in range(317)]))
test['CITY']  = test['CITY'].map(House_dict)

STATE = test['STATE'].value_counts().index
STATE_dict = dict(zip(STATE, [i for i in range(29)]))
test['STATE']  = test['STATE'].map(House_dict)

# 载入模型，完成预测
model = lgb.Booster(model_file='model.txt')
result=model.predict(test)

# 输出结果，保存到文件
submission=result
np.savetxt('submission(LGBM)2.csv',submission,fmt='%.15f')	

```

